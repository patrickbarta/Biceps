---
title: "Prediction of Proper and Incorrect Biceps Exercise Performance by Random Forests"
author: "Patrick Barta, patrickbarta@patrickbarta.com"
date: "February 23, 2016"
output: html_document
---
# Overview
In this project, mechanical sensor data from six subjects who performed 10 repetitions of a biceps exercise five different ways (correctly and four additional incorrect ways) were used to build a machine learning model for predicting whether an individual was performing the exercise properly, and, if incorrectly, what  kind of error was most likely. Using a random forest machine learning algorithm, the model performed very well, with a prediction accuracy near 99%.

# Processing environment
```{r, echo=FALSE, results='hide', message=FALSE}
knitr::opts_chunk$set(cache=TRUE)
require(gridExtra); require(caret); require(randomForest); require(e1071)
```
I used R version 3.2.3 under Ubuntu 15.10, with the `caret`, `gridExtra`, `randomForest` and `e1071` libraries loaded. The `caret` library loads the `ggplot` library as a dependency.

# Raw data and tidying
The sensor dataset was obtained from url's in the code below. These data are part of the [Human Activity Recognition project](http://groupware.les.inf.puc-rio.br/har) and described in the following publication: 
Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. *Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements*. **Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012**. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. The download url's provide elementary codebook information used to understand the data.

The following code was used to create the tidy data for this project. Comments relevant to each section of the code are given below.
```{r}
if (!(exists("tidyTrain") && exists("tidyTest"))) {
    #-------------------- Section 1 - Download and read files
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                  destfile="Train.csv", method="curl")
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                  destfile="Test.csv", method="curl")
    # below produces data frame with 160 columns and 19622 rows
    rawDataTrain <- read.csv("Train.csv", na.strings=c("NA", "")) 
    # below produces data frame with 160 columns and 20 rows
    rawDataTest <- read.csv("Test.csv", na.strings=c("NA", ""))   
    # uncomment table command below to get table showing that 60 columns of training set 
    # have no missing data, while 100 columns have 19216/19622 values missing.
    # table(colSums(is.na(rawDataTrain)))
    #-------------------- Section 2 - Get rid of columns that are almost all missing data
    goodColumns <- colSums(is.na(rawDataTrain)) == 0
    tempTrain <- rawDataTrain[,goodColumns]
    tempTest <- rawDataTest[,goodColumns]
    # in last column, 'classe' is the variable we want to predict in tidyTrain
    #--------------------  Section 3 - Get rid of columns that aren't features or class value
    tidyTrain <- tempTrain[,-c(1:7)]    # produces data frame with 53 columns annd 19622 rows
    tidyTest <- tempTest[,-c(1:7)]      # produces data frame with 53 columns and 20 rows
    #--------------------  Section 4 - Crosscheck for missing data
    # uncomment below to show there is now no missing data
    # sum(is.na(tidyTrain)) + sum(is.na(tidyTest))
}
```
The code above was written iteratively. The first two lines of code in Section 1 just download the data. On inspection of the data after downloading, it was clear that a significant amount of the data were missing--the cells were just empty--so the `na.string` argument of `read.csv` was added to ignore these cells. The commented `table` command, if executed, shows that 60 columns of the training set have no missing data, while 100 columns have 19216/19622 values missing. These 100 columns have so little data that they provide little information. Section 2 elimates these columns from both the testing and training sets. The first 7 columns of the data are not features, but information such as subject names and so forth, so these data were eliminated in Section 3. The commented code in Section 4, if executed, shows that there are no `NA`'s in the final tidy datasets. It is important to note that the original data appear to be organized as a time series, but my analysis here does not take this into account explicitly, instead just treating these data as independent samples from the feature set. The first 52 columns of the tidy datasets are mechanical features of the activty type such as roll, pitch, yaw, at 4 different locations: belt, arm, forearm and dumbbell. The last column of the training set, `classe`, shows the biceps activity being measured, with `A` denoting the "correct" activity, while the other levels are "incorrect". The last column of the test set is just an index of the Coursera question number.

# Exploratory data analysis
Exploratory data analysis was done with the following code: 
```{r}
# uncomment the following to show number of examples in each class: A=5580, B=3797, C=3422, D=3216, E=3607
# summary(tidyTrain$classe)
# Create a density plot of all predictor variables, faceted by classe
plt<-list(length=52)
for (i in 1:52) {
    plt[[i]]<-ggplot(tidyTrain, aes_string(x=names(tidyTrain)[i], color='classe')) + geom_density()
}
# uncomment next line for grid of all variables. Visible on big monitor, a mess in html.
# marrangeGrob(grobs=plt, nrow=13, ncol=4)
# this shows a sample of 3 interesting plots and one uninteresting one, suitable for html
grid.arrange(plt[[1]], plt[[41]], plt[[3]], plt[[31]], nrow=2)
```

The commented line beginning with `summary` shows that the different classes are relatively evenly distributed in the data. Density plots for all feature variables were created in the `plt` list. The commented line beginning with `marrange` produces a grid of plots showing the density histograms of all the feature variables, but does not show well when processed to html, so the uncommented line shows the density histograms in the figure above (faceted by `classe` type) for three "interesting" features and one "uninteresting" feature. "Interesting" features differ significantly between classes, while "uninteresting" ones do not.

# Training for prediction
There are many options for machine learning algorithms. In the original paper, the authors reported good prediction performance from a random forest algorithm, so I chose to try to reproduce that result. Given that they reported very good performance, it made sense to me to see if I could do the same, rather than experimenting with other algorithms. (If random forests work well, why reinvent the wheel?) Before using R's random forests algorithm to train the model, I split the tidy training set into `myTraining` and `myTest` datasets, with 60% in the `myTrain` set and 40% in the `myTest` set used to test the random forests model. I did 10-fold cross validation in the model training to balance bias versus variance. Further details of the final model (not shown) can be seen by uncommenting the last line of code.
```{r}
# partition tidyTest into a training and test set
set.seed(42)  # the canonical choice
trainingIndices <- createDataPartition(y = tidyTrain$classe, p = 0.6, list = FALSE)
myTrain <- tidyTrain[trainingIndices,]
myTest <- tidyTrain[-trainingIndices,]

# train model
myModel <- train(classe ~ ., data = myTrain, method = "rf", prox = TRUE, 
               trControl = trainControl(method = "cv", number = 10, allowParallel = TRUE))
# uncomment to get details of myModel
# myModel
```
# Diagnostics
Before proceeding to testing, I chose to do an importance plot to see the relative contributions from various predictor variables (note that top 3 correspond to 3 of the 4 sub-plots in the exploratory data analysis.) Basically, a large decrease in Gini coefficient corresponds to an important prediction variable.
```{r}
varImpPlot(myModel$finalModel)
```

# Testing
Now that we've build the model, we are most interested in its performance on the **test**, not the **training** set.
```{r}
# uncomment for accuracy on training set
#myTrainPredictions <- predict(myModel, myTrain)
#confusionMatrix(myTrainPredictions, myTrain$classe)

# accuracy on test set
myTestPredictions <- predict(myModel, myTest)
confusionMatrix(myTestPredictions, myTest$classe)
```
The confusion matrix shows very good performance. Almost everything is assigned to the correct class with an accuracy of 99%.

# Submission for quiz
For the quiz for this project, we use the same model to predict on another testing set.
```{r}
# figure out predictions for grading
courseraPredictions <- predict(myModel, tidyTest)
# print out predictions for grading purposes
as.character(courseraPredictions)
```

# Conclusions

Basically, the performance of the random forest prediction model was outstanding (99%)! It is important though to point out that the number of subjects in the study was small, and prediction might be substantially less impressive in different subjects. Prediction for another test set, composed of different subjects, would likely perform less well but that performance would likely yield a much better estimate of how well this model would perform in real life.


